# Getting Started — Urban Environmental Intelligence Engine

## Quick Start (2 minutes)

Run the pipeline with synthetic data:

```bash
python main.py
```

Start the interactive dashboard:

```bash
streamlit run dashboard.py
```

Then open your browser to `http://localhost:8501`.

---

## Full Setup (5 minutes)

### 1. Prerequisites

- pip or conda

### 2. Clone/Navigate to Project

````

### 3. Create Virtual Environment

**Windows (PowerShell):**

```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
````

**macOS/Linux:**

```bash
python -m venv venv
source venv/bin/activate
```

### 4. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## Running the Pipeline

### Mode 1: Synthetic Data (DEFAULT)

**Use this for fast testing.** Generates realistic but fake data — completes in ~15 seconds.

```bash
python main.py
```

**Output:**

- Processes 876,000 synthetic hourly readings from 100 stations
- Generates 7 PNG visualizations in `output/`

**Use this to fetch real air quality measurements from OpenAQ v3.** Takes 5–10 minutes depending on rate limits and API availability.

```bash
python main.py --live
```

#### What Happens:

1. **Fetches 100 air quality stations** from OpenAQ API v3
2. **Downloads hourly measurements** for each station (PM2.5, PM10, NO2, Ozone, Temperature, Humidity) for full year 2025
3. **Saves raw JSON responses** to `data/raw_openaq/` (for auditing)
4. **Combines & interpolates** the data (fills gaps where sensors went offline)
5. **Runs all 4 analytical tasks** on real data
6. **Generates visualizations** with real measurements
7. **Saves processed data** to `data/sensor_data.parquet` (same format as synthetic)

#### API Key Configuration:

The code reads your API key from the environment variable `OPENAQ_API_KEY`. If not set, it uses a demo/embedded key and prints a warning.

**Set your own API key:**

**Windows (PowerShell):**

```powershell
setx OPENAQ_API_KEY "your_real_openaq_api_key_here"
```

Then restart your terminal or Python environment.

**macOS/Linux (Bash/Zsh):**

```bash
export OPENAQ_API_KEY="your_real_openaq_api_key_here"
python main.py --live
```

Or add to your shell profile (`~/.bashrc`, `~/.zshrc`):

```bash
echo 'export OPENAQ_API_KEY="your_real_openaq_api_key_here"' >> ~/.bashrc
source ~/.bashrc
```

**Get your real API key:**

- Visit [OpenAQ Platform](https://platform.openaq.org/)
- Sign up (free)
- Generate an API key in your profile settings

---

## Running the Dashboard

After running `python main.py` or `python main.py --live`, the data is saved to `data/sensor_data.parquet`.

```
  You can now view your Streamlit app in your browser.

```

**Dashboard Features:**

- **4 Interactive Tabs** (one per analytical task)
- **Filters:** Zone (Industrial/Residential), Region selection
- **Metrics:** Violation counts, extreme event counts, variance explained
- **Interactive Charts:** Zoom, hover, pan on all plots
- **Real-time Updates:** Filters apply instantly

**Stop the dashboard:**
Press `Ctrl+C` in the terminal.

---

## Understanding the Data Flow

```
┌─────────────────────────────────────────┐
│ python main.py [--live]                 │
│                                         │
│  [Synthetic OR Real OpenAQ Data] → 876k rows
│                  ↓                       │
│  [Data Pipeline: Clean, Interpolate]    │
│                  ↓                       │
│  [4 Tasks: PCA, Temporal, Distrib, ...] │
└─────────────────────────────────────────┘
    ┌──────────────────────────────┐
    │ streamlit run dashboard.py   │
    │                              │
    │ [Load Parquet] → [Display]   │
    │ http://localhost:8501        │
    └──────────────────────────────┘
```

**Key Point:** The dashboard does **NOT** make live API calls. It reads the Parquet file that was generated by `main.py`. So the data shown on the dashboard depends on which mode you ran:

- If you ran `python main.py` → dashboard shows **synthetic data**
- If you ran `python main.py --live` → dashboard shows **real OpenAQ data**

---

## Output Files

After running the pipeline, you'll find:

```
project_root/
│   ├── sensor_data.parquet          ← processed data (876k rows)
│   ├── station_metadata.parquet     ← station names, zones, regions
│   └── raw_openaq/                  ← (only with --live) raw API responses
├── output/
│   ├── task1_pca_biplot.png         ← PCA visualization
│   ├── task2_heatmap.png            ← Temporal heatmap (100 sensors)
│   ├── task2_periodic_signature.png ← Daily & monthly patterns
│   ├── task3_kde_peaks.png          ← Distribution (peak-optimized)
│   ├── task3_log_histogram.png      ← Distribution (tail-optimized)
│   ├── task4_small_multiples.png    ← 10 regions faceted view
│   └── task4_bivariate_map.png      ← Alternative 3D rejection
```

---

## Troubleshooting

### Issue: "OPENAQ_API_KEY not set" warning when running `--live`

### Issue: Dashboard won't start

**Ensure Streamlit is installed:**

```bash
pip install streamlit
```

Then:

````bash
streamlit run dashboard.py

### Issue: Live fetch is slow or times out

**Normal behavior.** The API is rate-limited (55 req/min) and some sensors have no data.

```bash
````

Then start the dashboard:

```bash
streamlit run dashboard.py
```

---

## Configuration

Edit `config.py` to adjust:

### Step 1: Open PowerShell

Press `Win + X` → Select "Windows PowerShell"

### Step 2: Navigate to Project

```powershell
cd "c:\Users\DELL\Desktop\6th Semester Tasks\Data Science\DataScience_Assignment2"
```

### Step 3: Activate Virtual Environment

```powershell
.\venv\Scripts\Activate.ps1
```

You should see `(venv)` at the start of your prompt.

### Step 4: Install Dependencies

```powershell
pip install -r requirements.txt
```

### Step 5: Run Pipeline (Choose One)

**For fast testing (synthetic data ~ 15 seconds):**

```powershell
# if you want to start fresh or previous live run was incomplete,
# delete the existing parquet file first:
Remove-Item data\sensor_data.parquet
python main.py           # now generates fresh synthetic data
```

**For real data from OpenAQ (5-10 minutes):**

```powershell
python main.py --live
```

Wait for it to finish. You'll see progress messages and generated PNG files will appear in the `output/` folder.

### Step 6: Start Dashboard

```powershell
streamlit run dashboard.py
```

Your browser will open automatically at `http://localhost:8501`. If not, paste that URL manually.

### Step 7: Explore

- Click the tabs (Task 1, Task 2, Task 3, Task 4)
- Use filters to explore data
- Zoom/pan on charts

### Step 8: Stop Dashboard

Press `Ctrl + C` in PowerShell.

---

- `PM25_HEALTH_THRESHOLD` — violation threshold (default: 35 μg/m³)
- `PM25_EXTREME_HAZARD` — extreme event threshold (default: 200 μg/m³)

---

## Next Steps

1. **Run synthetic:** `python main.py` (quick test)
2. **View outputs:** Check `output/` folder for PNG visualizations
3. **Explore dashboard:** `streamlit run dashboard.py`
4. **Try live data:** `python main.py --live` (requires API key if you want your own)
5. **Publish:** push the repository to GitHub (don't include `venv/`, `data/`, etc.).
   After pushing, link the repo in Streamlit Community Cloud and set the app command to `streamlit run dashboard.py`.
6. **Submit:** Ensure `PUBLIC_CONTENT.md` has your published Medium & LinkedIn links

---

## Need Help?

Check these files:

- `README.md` — Project overview
- `PROJECT_ASSESSMENT.md` — Detailed requirements analysis
- `FIXES_APPLIED.md` — Recent improvements
- Log output from `python main.py` — Detailed execution trace
